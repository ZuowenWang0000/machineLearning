\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}

\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{%
  \LARGE ENGG 5103 Course Project Report\\
  \Large  a study about text classification
  }
\author{
  Zuowen Wang\\
  \texttt{1155123906@link.cuhk.edu.hk} \\
  \and
  Xinyuan Zhang\\
  \texttt{1155123300@link.cuhk.edu.hk} \\
}
\date{November 2018}
% TODO

% 8. References

\begin{document}
\maketitle
\newpage
\begin{abstract}
Abstract section
\end{abstract}
% 1. Introduction and related work
%   a) what is the problem you want to solve?
%   b) why is the problem important?
%   c) what are the previous solutions by others?
%   d) what you did to solve this problem?
%   e) how are your results compared with others?
%   f) what is the structure of this report?
\section{Introduction and Related Work}
\subsection{What is the problem you want to solve}
Due to the rapid development of social network these years, short to medium length texts are gradually dominating the Internet. Various user backgrounds on large social networks such as Twitter and Facebook result in difficulties recommending users the content they would like to view. One way to satisfy this demand is to extract ‘class’ from texts, namely text classification. For this project, we are going to 
\subsection{Why is the problem important}
Then based on the summarized topic information, we can do further meaningful operations accordingly
\subsection{What are the previous solutions by others}
Various previous works have tried to tackle this problem and very successful results have been achieved. This problem is fundamentally a natural language processing problem so all the feature extraction and classification methods are inherited from these well studied domains.

Sriam, Fuhry, Demir, Ferhatosmanoglu and Demirbas \cite{SFDFD} use domain specific features extracted from the Twitter author's profile and text. They train and test their algorithm on a predefined set of generic classes such as News, Events, Opinions, Deals, and Private Messages. Lee, Palsetia, Narayanan, Patwary, Agrawal and Choudhary proposed in their work \cite{LPNPAC} an innovative network based approach apart from their study using classical natural language processing methods. Chen, Jin and Shen conduct their work based on the classical latent Dirichlet allocation model\cite{BNJ} by learning multi-granularity topics.\cite{CJS}
\subsection{What you did to solve this problem}
TODO
\subsection{How are your results compared with others}
TODO
\subsection{What is the structure of this report}
TODO


% 2. Problem description
%   a) define your problem formally and precisely.
%   b) what is the goal of the project?
\section{Problem description}
\subsection{formal definition of our problem}
TODO
\subsection{What is the goal of the project}
TODO

% 3. Data set and preprocessing
%   a) what is the data source?
%   b) show statistics of the raw data.
%   c) elaborate the preprocessing steps explicitly.
%   d) show statistics of the preprocessed data.
\section{Data set and preprocessing}
\subsection{what is the data source?}
We use three datasets, namely R8, R52 and 20 Newsgroups. The first two datasets are processed based on the Reuters 21578 datasets\cite{HB}, which is a well studied and broadly used datasets in the field of natural language processing and data mining. R8 dataset includes 8 classes and R52 contains 52 classes. Notice that the documents are not very evenly distributed among all the categories.

Compare to the Reuters dataset, the 20 Newsgroups is a relatively new dataset. It is a collection of approximately 20,000 newsgroup documents, partitioned evenly across
20 different categories.

All documents are divided into train/test datasets when we download them. And all of them are already preprocessed with algorithms provided by the author \cite{myself}. In the later sections we will briefly introduce her preprocessing methods. Also we will show the statistics of datasets before and after preprocessing.
\subsection{show statistics of the raw data}
\subsubsection{the Reuters-21578 dataset}
The Reuters-21578 dataset is stored in 22 files. For the first 21 files each contains 1000 documents, while the last file contains 578 documents. All files are in SGML format.

Each document contains the following tag at the very beginning:
\bigbreak
\textbf{<REUTERS TOPICS=?? LEWISSPLIT=?? CGISPLIT=?? OLDID=?? NEWID=??>}
\bigbreak
And each document ends with:
\bigbreak
\textbf{</REUTERS>}
\bigbreak

In the header of each file we care more about the topics information. Each document can have multiple topics. But for our training purpose we only do single label classification. We will mention later in the section 3.4 how we solve this problem.\par The dataset is divided into the 5-fold fashion for training and validation. The important statistics about the raw dataset is shown below. Notice that we will discard all documents with no label.
\bigbreak
\bigbreak
\begin{table}[h]
\centering
\begin{tabular}{ccccc}
\multicolumn{5}{c}{\textbf{Number of documents}}                       \\
         & \textbf{Total} & \textbf{No label}     & \textbf{Unilabel}     & \textbf{Multi-Label}  \\
\textbf{Training} & 9603  & 1828(19.0\%) & 6552(68.3\%) & 1223(12.7\%) \\
\textbf{Test}     & 3299  & 280(8.5\%)   & 2581(78.2\%) & 438(13.3\%) 
\end{tabular}
\caption{statistics of Reuters-21578 data set}
\label{my-label}
\end{table}
\bigbreak

\subsubsection{the 20 Newsgroups data set}
The 20 Newsgroups data set is categorized into 20 different news types, each corresponding to a single topic label, which makes it very useful for our task. The topics and text material contained in this data set is relatively newer and not totally about politics or economics so it might be more helpful for our purpose, since we are testing the model on twitter text, the topics are more likely to be less serious. Although the situation really depends on who the authors are.

All documents in the 20 Newsgroups data set is partitioned into 5 folds and 2 folds are in the test set. The original data set contains 19997 text documents but the data set we use is already pre-processed by removing all the duplicates. Moreover, only the author and subject are kept in the header. Thus we got 18828 documents left.

Some of the topics are highly correlated to each other while others are totally unrelated.\cite{20news} We show the 20 news types below and partition them into different groups according to their topics.

\bigbreak
\bigbreak
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\begin{tabular}[c]{@{}l@{}}comp.graphics\\ comp.os.ms-windows.misc\\ comp.sys.ibm.pc.hardware\\ comp.sys.mac.hardware\\ comp.windows.x\end{tabular} & \begin{tabular}[c]{@{}l@{}}rec.autos\\ rec.motorcycles\\ rec.sport.baseball\\ rec.sport.hockey\end{tabular} & \begin{tabular}[c]{@{}l@{}}sci.crypt\\ sci.electronics\\ sci.med\\ sci.space\end{tabular}         \\ \hline
misc.forsale                                                                                                                                        & \begin{tabular}[c]{@{}l@{}}talk.politics.misc\\ talk.politics.guns\\ talk.politics.mideas\end{tabular}      & \begin{tabular}[c]{@{}l@{}}talk.religion.misc\\ alt.atheism\\ soc.religion.christian\end{tabular} \\ \hline
\end{tabular}
\caption{category groups in the 20 Newsgroups data set}
\label{my-label2}
\end{table}

\bigbreak
\subsection{elaborate the preprocessing steps explicitly}
\subsubsection{Preprocessing steps for the Reuters data set}
For the Reuters 21578 data set we used a version which has already been preprocessed. Thus here we will introduce the method which the author used.
\bigbreak
\cite{myself} The preprocessing steps are listed as follow:
    \begin{enumerate}
    \item \textbf{all-terms} Obtained from the original datasets by applying the following transformations:
    \begin{itemize}
        \item Substitute TAB, NEWLINE and RETURN characters by SPACE.
        \item Keep only letters (that is, turn punctuation, numbers, etc. into SPACES).
        \item Turn all letters to lowercase.
        \item Substitute multiple SPACES by a single SPACE.
        \item The title/subject of each document is simply added in the beginning of the document's text.
    \end{itemize}
    
    \item \textbf{no-short} Obtained from the previous file, by removing words that are less than 3 characters long. For example, removing "he" but keeping "him".
    \item \textbf{no-stop} Obtained from the previous file, by removing the 524 SMART stopwords. Some of them had already been removed, because they were shorter than 3 characters.
    \item \textbf{stemmed} Obtained from the previous file, by applying Porter's Stemmer to the remaining words. Information about stemming can be found here.
\end{enumerate}

We also want to add some our own comments here about the above preprocessing procedure. One major flaw is words like \textbf{\textit{the U.S.}} will be processed as \textbf{\textit{the u s}}. And if we further filter out all words with length shorter than a certain threshold, which is a very common approach in natural language processing, then we will lose a major information. Thus we prefer to define certain corner cases to keep words like \textbf{\textit{the U.S.}} as \textbf{\textit{US}} and do not delete them in later steps of the pipeline. Another issue is the stop word list the author uses might not be ideal for our purpose. But since we do not want to make our task too complicated we will stick to the stopwords list which the author uses. 

\subsubsection{Preprocessing steps for the 20 Newsgroups data set}

TODO


\subsection{show statistics of the preprocessed data}


% 4. Algorithms
%   a) introduce the overview of your project.
%   b) elaborate how each algorithm is applied.
%   c) state the reasons why you choose those algorithms
\section{Algorithms}
\subsection{introduce the overview of your project}
TODO
\subsection{elaborate how each algorithm is applied}
TODO
\subsection{state the reasons why you choose those algorithms}


% 5. Results and Analysis
%   a) use tables and graphs.
%   b) use quantitative values.
%   c) do the results match your expectation and why?
%   d) what knowledge have you learned from the results?
%   e) limitations and further improvement of your project.
\section{Results and Analysis}
\subsection{}
% * <wangzu@student.ethz.ch> 2018-11-26T11:02:40.895Z:
%
% ^.
% 6. Conclusion
\section{Conclusion}


% 7. Task Allocation
%   a) contribution of each group member
\section{Task Allocation}



\subsection{How to add Tables}


\subsection{How to write Mathematics}

\LaTeX{} is great at typesetting mathematics. Let $X_1, X_2, \ldots, X_n$ be a sequence of independent and identically distributed random variables with $\text{E}[X_i] = \mu$ and $\text{Var}[X_i] = \sigma^2 < \infty$, and let
\[S_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
      = \frac{1}{n}\sum_{i}^{n} X_i\]
denote their mean. Then as $n$ approaches infinity, the random variables $\sqrt{n}(S_n - \mu)$ converge in distribution to a normal $\mathcal{N}(0, \sigma^2)$.


\subsection{How to create Sections and Subsections}

Use section and subsections to organize your document. Simply use the section and subsection buttons in the toolbar to create them, and we'll handle all the formatting and numbering automatically.

\subsection{How to add Lists}


\subsection{How to add Citations and a References List}



You can find a \href{https://www.overleaf.com/help/97-how-to-include-a-bibliography-using-bibtex}{video tutorial here} to learn more about BibTeX.

We hope you find Overleaf useful, and please let us know if you have any feedback using the help menu above --- or use the contact form at \url{https://www.overleaf.com/contact}!

\bibliographystyle{alphaurl}
\bibliography{references}


\end{document}
